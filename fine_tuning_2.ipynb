{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPT8o/in2/Sdk5fr5Sm3uCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/callaghanmt-training/ou-fine-tuning-2025-11/blob/main/fine_tuning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 2: Fine Tuning using Unsloth\n",
        "In this notebook, we will fine-tune an SLM using the (Q)LoRA methodology and the [Unsloth](https://docs.unsloth.ai/) library.\n",
        "\n",
        "Unsloth is generally a good choice for resource-contrained environments and 'beginners' as it is optimised to make training faster, use less VRAM and abstracts away mauch of the complexity of training with the standard Huggingface ecosystem.  We _do_ lose some flexibility this way, but it will allow us to get meaningful results in a much shorter space of time.\n",
        "\n",
        "##Before you start:\n",
        "1. Make sure you have connected to a T4 runtime and clicked the **[Connect]** button\n",
        "2. Add your Huggingface Access Token to the notebook 'secrets'\n",
        "\n",
        "##Notebook Overview\n",
        "1. **Setup**: Install Unsloth and dependencies.\n",
        "1. **Model Loading**: Load Gemma 2 2B (IT version) in 4-bit quantization (QLoRA).\n",
        "1. **Data Prep**: We will first create a synthetic \"Stoic Wisdom\" dataset right in the notebook so you don't have to rely on external file uploads during the initial part of the workshop (Optionally - do the fine tuning run again with a larger external dataset).\n",
        "1. **Configuration**: Set up LoRA adapters.\n",
        "1. **Training**: Run the fine-tuning process.\n",
        "1. **Inference**: Test the new \"Philosopher\" personality.\n",
        "1. **Energy Use**: Run the fine-tuning loop again, this time wrapped with the `codecarbon` libary to get an idea of the energy/ carbon footprint of fine-tuning.\n"
      ],
      "metadata": {
        "id": "vQNKh5Nyx2cS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installation\n",
        "Unsloth requires a specific installation order to use the GPU kernels."
      ],
      "metadata": {
        "id": "GLg0Wv5azuHw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw9RvRp2xuUb"
      },
      "outputs": [],
      "source": [
        "# Confirm we have access to an NVidia GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Unsloth and dependencies for Colab (2-3 minutes for this cell)\n",
        "# We use the specific Colab install command provided by Unsloth\n",
        "!pip install --quiet \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# 2. Install Hugging Face libraries\n",
        "#!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "hSlSfNvvz-Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps --quiet xformers trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "8Owp3ggu9_ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Imports and Configuration\n",
        "We set up the parameters below. We are using a `max_seq_length` of 2048, which is standard and efficient."
      ],
      "metadata": {
        "id": "w-eCTdj80PMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None # None = auto detection. Float16 for Tesla T4, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage\n",
        "\n",
        "print(\"Libraries loaded and configuration set.\")"
      ],
      "metadata": {
        "id": "bu4GDU0H0EDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3: Load Model and Tokenizer\n",
        "We load the **Instruction Tuned** version of Gemma 2.\n",
        "\n",
        "_Why fine-tune the IT version?_ It already understands chat structure (it's a _language engine_). We are just \"steering\" its style from \"Generic Assistant\" to \"Stoic Philosopher.\"\n",
        "\n",
        "(This bit takes ~3 minutes)"
      ],
      "metadata": {
        "id": "_J0Awunl1S7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"Gemma 2 2B (4-bit) loaded successfully.\")"
      ],
      "metadata": {
        "id": "IWGSpB2Z1gbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4: Data Preparation (The \"Stoic\" Dataset)\n",
        "For the speedy version of this workshop, creating data on the fly is safer than downloading files. Here we define a list of modern problems and map them to **Stoic** responses (Marcus Aurelius/Seneca style)."
      ],
      "metadata": {
        "id": "r85F_Ptg0g_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a system prompt to guide the style\n",
        "system_prompt = \"\"\"You are a Stoic philosopher in the style of Marcus Aurelius and Seneca.\n",
        "Answer the user's questions with wisdom, focusing on what is within their control,\n",
        "the nature of the universe, and the importance of virtue over external events.\n",
        "Keep your tone calm, archaic, and reflective.\"\"\"\n",
        "\n",
        "# Synthetic Data: Modern problems -> Stoic solutions\n",
        "stoic_data = [\n",
        "    {\"instruction\": \"I am stuck in traffic and I'm going to be late!\", \"output\": \"Why do you let external events disturb your inner peace? The traffic is not within your control. Your reaction to it is. Accept this moment as nature intends. Use this time for reflection rather than anger.\"},\n",
        "    {\"instruction\": \"My boss yelled at me today.\", \"output\": \"If your boss acted with reason, learn from it. If they acted with anger, that is their harm, not yours. Do not let another's lack of self-control destroy your own. You can only be harmed if you believe you are harmed.\"},\n",
        "    {\"instruction\": \"I want to buy a new expensive car to impress my neighbors.\", \"output\": \"Desire is a chain. You seek to impress those who do not care for you with things you do not need. True wealth is found in the want of nothing, not the possession of much. Seek virtue, not applause.\"},\n",
        "    {\"instruction\": \"I am worried about the future.\", \"output\": \"The future is uncertain, but your character is constant. Do not suffer before it is necessary. If the future brings hardship, you will face it with the same weapons of reason you possess today.\"},\n",
        "    {\"instruction\": \"Someone insulted me on the internet.\", \"output\": \"The insult exists only if you accept it. A rock thrown into the air gains nothing by going up and loses nothing by falling down. Your value is not determined by the opinions of strangers.\"},\n",
        "    {\"instruction\": \"I feel overwhelmed by my todo list.\", \"output\": \"Do not confuse activity with action. Focus on the task at hand, as if it were the last thing you were doing in your life. Do it with dignity and without distraction. The rest will follow.\"},\n",
        "    # ... In a real scenario, we would want 100+ examples.\n",
        "    # For this workshop demo, we will duplicate these to simulate a training run.\n",
        "] * 10\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_list(stoic_data)\n",
        "\n",
        "# Function to format the data into the chat structure Gemma expects\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for instruction, output in zip(examples[\"instruction\"], examples[\"output\"]):\n",
        "        # We use the tokenizer's chat template\n",
        "        # Note: We inject the system prompt implicitly by how we structure the response\n",
        "        text = tokenizer.apply_chat_template([\n",
        "            {\"role\": \"user\", \"content\": instruction},\n",
        "            {\"role\": \"model\", \"content\": output}\n",
        "        ], tokenize=False, add_generation_prompt=False)\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "print(f\"Dataset created with {len(dataset)} examples.\")\n",
        "print(\"Example formatted entry:\", dataset[0][\"text\"])"
      ],
      "metadata": {
        "id": "F2sHKCrv1sgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Setting up LoRA (Low-Rank Adaptation)\n",
        "We don't retrain the whole model (that's too expensive). We add small \"adapter\" layers.\n",
        "\n",
        "`r`: The rank. Higher = more parameters to train (slower, maybe smarter). 16 is standard.   \n",
        "`target_modules`: The specific internal layers of the model we are modifying (all of this ios in the documentation as the architecture is 'known')"
      ],
      "metadata": {
        "id": "DlhBFEHi17lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimised\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimised\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # Rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters attached.\")"
      ],
      "metadata": {
        "id": "AhRfBSY72Pcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6: Training (The Fine-Tuning)\n",
        "We use the `SFTTrainer` function.\n",
        "\n",
        "`max_steps`: For this workshop, set this to 60. It takes about 2-3 minutes. In real life, we'd train for multiple epochs.   \n",
        "`learning_rate`: `2e-4` is the standard \"magic number\" for QLoRA (again, from the literature).\n",
        "\n",
        "(The training loop takes about 3-4 minutes)"
      ],
      "metadata": {
        "id": "bm5plhHP2e8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a basic LORA training function\n",
        "def lora_loop():\n",
        "    trainer = SFTTrainer( #HF Supervised Fine Tuning Trainer\n",
        "        model = model,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = dataset,\n",
        "        dataset_text_field = \"text\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        dataset_num_proc = 2,\n",
        "        packing = False, # Can make training faster for large datasets\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = 2,\n",
        "            gradient_accumulation_steps = 4,\n",
        "            warmup_steps = 5,\n",
        "            max_steps = 60, # Set to 60 for a quick workshop demo!\n",
        "            learning_rate = 2e-4,\n",
        "            fp16 = not torch.cuda.is_bf16_supported(),\n",
        "            bf16 = torch.cuda.is_bf16_supported(),\n",
        "            logging_steps = 1,\n",
        "            optim = \"adamw_8bit\",\n",
        "            weight_decay = 0.01,\n",
        "            lr_scheduler_type = \"linear\",\n",
        "            seed = 3407,\n",
        "            output_dir = \"outputs\",\n",
        "            # Disable wandb logging\n",
        "            report_to = \"none\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer_stats = trainer.train()\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "C2bTWvmC2zTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the training loop\n",
        "lora_loop()"
      ],
      "metadata": {
        "id": "BUrrnNnz7weI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7: Inference (Testing the Philosopher)\n",
        "Now we test the model. We need to use `FastLanguageModel.for_inference` to enable the optimised inference speeds.\n",
        "\n"
      ],
      "metadata": {
        "id": "x3IZ1zXz28VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "\n",
        "# A new question, not in the training data\n",
        "prompt = \"I've lost my car keys. Again\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add this to signal the model to generate\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids = inputs,\n",
        "    max_new_tokens = 128,\n",
        "    use_cache = True,\n",
        "    temperature = 0.7 # Add a little creativity\n",
        ")\n",
        "\n",
        "# Decode the response\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"<start_of_turn>model\")[-1]) # Clean up output"
      ],
      "metadata": {
        "id": "MaKR7vOc3F4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If we get an `attention mask` warning it is irritating. The warning occurs because the model's\n",
        "# configuration has the Pad Token set to the same ID as the EOS (End of Sentence) Token.\n",
        "# When we pass raw input IDs without an explicit mask, the model gets confused about\n",
        "# where the actual sentence ends and where \"padding\" begins.\n",
        "\n",
        "# Fix: manually create an attention mask of all 1s and pass it to `model.generate`.\n",
        "\n",
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# A new question, not in the training data\n",
        "prompt = \"I lost my car keys and I am very angry.\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# FIX: Create an attention mask of 1s (since all tokens are valid)\n",
        "attention_mask = torch.ones_like(inputs)\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids = inputs,\n",
        "    attention_mask = attention_mask, # <--- Pass the mask here\n",
        "    max_new_tokens = 128,\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,\n",
        "    pad_token_id = tokenizer.eos_token_id # Optional: Explicitly set pad_token_id to silence other potential warnings\n",
        ")\n",
        "\n",
        "# Decode the response\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"<start_of_turn>model\")[-1])"
      ],
      "metadata": {
        "id": "TylpAsTm551r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we like, we can do a 'before and after' comparison of this model with and without the adapters.  We would probably want to do this in a workshop to confirm to participants that we have actually done something!"
      ],
      "metadata": {
        "id": "2RCthJUC_81Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# 1. Optimise the model for inference (Run this once)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Define your prompt\n",
        "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "I lost my car keys and I am very angry\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# Setup streamer for live output\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "print(\"=========================================\")\n",
        "print(\"ORIGINAL MODEL\")\n",
        "print(\"=========================================\")\n",
        "\n",
        "# 2. Use the context manager to DISABLE LoRA temporarily\n",
        "# This tells the model to ignore the fine-tuned weights and use the base weights\n",
        "with model.disable_adapter():\n",
        "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=128)\n",
        "\n",
        "print(\"\\n\\n=========================================\")\n",
        "print(\"FINE-TUNED MODEL (LoRA)\")\n",
        "print(\"=========================================\")\n",
        "\n",
        "# 3. Run normally (LoRA adapters are active by default)\n",
        "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=128)"
      ],
      "metadata": {
        "id": "65IMhROH_7sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8: Saving the Model\n",
        "In a real workflow, we would save the LoRA adapters to merge them later or load them for inference."
      ],
      "metadata": {
        "id": "TsYl3zJ83Rm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the LoRA adapters locally\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "\n",
        "print(\"Adapters saved to 'lora_model' directory.\")\n",
        "\n",
        "# Optional: Push to Hugging Face Hub\n",
        "# model.push_to_hub(\"your_hf_username/gemma-2-2b-stoic-lora\")"
      ],
      "metadata": {
        "id": "XvRBMfq_3Q2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've pushed a copy of [my model](https://huggingface.co/callaghanmt/gemma-2-2b-stoic-lora) to my Huggingface repo. You can use this for the last part of the workshop (evaluation) if you don't want to use your own model."
      ],
      "metadata": {
        "id": "ybBtvmIKCZ-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Examining the energy use of this fine-tuning run\n",
        "The best tool to demonstrate this is in a workshop is [CodeCarbon](https://codecarbon.io/).   \n",
        "It wraps around the LoRA training loop and generates a report."
      ],
      "metadata": {
        "id": "y1UWIR6v6s9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet codecarbon\n",
        "# Errors here with some dependency conflicts don't prevent codecarbon from working"
      ],
      "metadata": {
        "id": "npDb7eJV7RH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Where is this notebook running?\n",
        "!curl -s ipinfo.io"
      ],
      "metadata": {
        "id": "RdJpzDvUI5yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from codecarbon import EmissionsTracker\n",
        "import time\n",
        "\n",
        "# Initialise the tracker\n",
        "# output_dir=\".\" saves the emissions.csv to your file browser panel\n",
        "tracker = EmissionsTracker(project_name=\"LoRA_FineTuning_T4\", output_dir=\".\")\n",
        "\n",
        "tracker.start()\n",
        "\n",
        "try:\n",
        "    # --- THE LORA TRAINING CODE GOES HERE ---\n",
        "    lora_loop()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # This stops the tracker and saves the data even if code errors out\n",
        "    emissions = tracker.stop()\n",
        "    print(f\"Training complete. Emissions: {emissions} kg CO2eq\")"
      ],
      "metadata": {
        "id": "-aSEdoRy7au8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
