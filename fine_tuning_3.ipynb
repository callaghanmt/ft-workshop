{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXXyUgAiVOqZWI11NmqKyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/callaghanmt-training/ou-fine-tuning-2025-11/blob/main/fine_tuning_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 3: Domain Specific Evaluation\n",
        "\n",
        "This notebook implements a **Domain-Specific Benchmark**. Since general benchmarks (like maths or coding tests) won't measure how \"Stoic\" a model is, we will build a custom \"Stoic Score\" metric.\n",
        "\n",
        "This script compares the Base Model (Gemma 2) against the Fine-Tuned Model (Stoic Gemma) on a set of life scenarios, scoring them based on key philosophical terminology.\n",
        "\n",
        "**Note:** This is a 'taster' appropach for the purposes of this workshop.  In production, we would need a number of more rigorous evaluations.  Lots more in the literature, including this [recent paper](https://arxiv.org/pdf/2506.12958)."
      ],
      "metadata": {
        "id": "AJTcIPHsD_Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Before you start:\n",
        "1. Make sure you have connected to a T4 runtime and clicked the **[Connect]** button\n",
        "2. Add your Huggingface Access Token to the notebook 'secrets'"
      ],
      "metadata": {
        "id": "G0v86uxnEOyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The \"Original\" Model**: We use this with `model.disable_adapter()`:. This allows us to see what Gemma 2 would have said without our training. It usually gives generic, empathetic advice (e.g., \"I'm sorry to hear that, try updating your resume\").\n",
        "\n",
        "**The \"Fine-Tuned\" Model**: This runs with the LoRA adapters active. It should shift the tone to use words like \"control,\" \"virtue,\" and \"mind.\"\n",
        "\n",
        "**The Metric**: We create a `calculate_stoic_score` function. In professional settings, this is often done using \"LLM-as-a-Judge\" (asking GPT-5 (or similar) to grade the response), but for this workshop, a keyword heuristic is fast, free, and effectively demonstrates the concept of alignment evaluation."
      ],
      "metadata": {
        "id": "w6tCQPl5Ea-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PART 1: SETUP & INSTALLATION\n",
        "# ==========================================\n",
        "# Install Unsloth for fast inference and memory management.\n",
        "# Note: If you are running this in a session where Unsloth is already installed,\n",
        "# you can comment out the pip install line.\n",
        "try:\n",
        "    import unsloth\n",
        "except ImportError:\n",
        "    !pip install --quiet \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "    !pip install --quiet --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n"
      ],
      "metadata": {
        "id": "uSeRf4QDE1qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kGPQrRq8G1hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PART 2: CONFIGURATION\n",
        "# ==========================================\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# CHANGE THIS LINE TO USE YOUR OWN MODEL\n",
        "# If you haven't uploaded one, use the workshop default below:\n",
        "# ------------------------------------------------------------------------\n",
        "MODEL_NAME = \"callaghanmt/gemma-2-2b-stoic-lora\"\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None # None for auto detection\n",
        "load_in_4bit = True # Use 4bit quantisation to fit in T4 GPU\n",
        "\n",
        "print(f\"‚è≥ Loading model: {MODEL_NAME}...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Optimise for inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"‚úÖ Model loaded and optimised.\")"
      ],
      "metadata": {
        "id": "AjrRhZTAFC5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PART 3: DEFINING THE EVALUATION DATASET\n",
        "# ==========================================\n",
        "# We need a set of \"prompts\" that trigger philosophical advice.\n",
        "# These are the \"Test Set\".\n",
        "\n",
        "eval_prompts = [\n",
        "    \"I lost my job today and I feel like a failure.\",\n",
        "    \"Someone insulted me on social media and I am furious.\",\n",
        "    \"I am worried about the future and things I cannot predict.\",\n",
        "    \"My car broke down on the way to an important meeting.\",\n",
        "    \"I want to be rich and famous, but I am not succeeding.\"\n",
        "]"
      ],
      "metadata": {
        "id": "K7ZtZCzBFcwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmaUlRglD8a5"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# PART 4: DEFINING THE METRIC (The \"Stoic Score\")\n",
        "# ==========================================\n",
        "# In a formal evaluation, we need a metric.\n",
        "# Since we don't have a 'Ground Truth' text, we will use a Heuristic Metric.\n",
        "# We check for the presence of specific Stoic concepts.\n",
        "\n",
        "STOIC_KEYWORDS = [\n",
        "    \"control\", \"virtue\", \"reason\", \"nature\", \"accept\",\n",
        "    \"indifferent\", \"mind\", \"reaction\", \"choice\", \"logos\",\n",
        "    \"opinion\", \"external\", \"power\", \"character\"\n",
        "]\n",
        "\n",
        "def calculate_stoic_score(text):\n",
        "    \"\"\"\n",
        "    Calculates a simple heuristic score based on keyword density.\n",
        "    Returns a score (0-100) representing 'Stoic-ness'.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    hits = 0\n",
        "    for word in STOIC_KEYWORDS:\n",
        "        if word in text:\n",
        "            hits += 1\n",
        "\n",
        "    # Normalize: If we find 4+ keywords, we consider it a \"High\" score.\n",
        "    score = min((hits / 4) * 100, 100)\n",
        "    return score, hits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PART 5: RUNNING THE EVALUATION LOOP\n",
        "# ==========================================\n",
        "# We will generate responses twice for each prompt:\n",
        "# 1. Using the BASE model (Adapters Disabled)\n",
        "# 2. Using the FINE-TUNED model (Adapters Enabled)\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"\\nüöÄ Starting Evaluation Loop...\\n\")\n",
        "\n",
        "# Format prompt for Gemma\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "for question in tqdm(eval_prompts, desc=\"Evaluating\"):\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        [alpaca_prompt.format(question)],\n",
        "        return_tensors = \"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # --- A. Run Base Model (Original Gemma) ---\n",
        "    # We disable the LoRA adapter to see how the generic model responds\n",
        "    with model.disable_adapter():\n",
        "        out_base = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
        "        decoded_base = tokenizer.batch_decode(out_base)[0].split(\"### Response:\\n\")[-1].replace(\"<eos>\", \"\").strip()\n",
        "\n",
        "    # --- B. Run Fine-Tuned Model (Stoic Gemma) ---\n",
        "    # Adapters are active by default\n",
        "    out_ft = model.generate(**inputs, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id)\n",
        "    decoded_ft = tokenizer.batch_decode(out_ft)[0].split(\"### Response:\\n\")[-1].replace(\"<eos>\", \"\").strip()\n",
        "\n",
        "    # --- C. Score Both ---\n",
        "    score_base, hits_base = calculate_stoic_score(decoded_base)\n",
        "    score_ft, hits_ft = calculate_stoic_score(decoded_ft)\n",
        "\n",
        "    results.append({\n",
        "        \"Scenario\": question,\n",
        "        \"Base_Response_Snippet\": decoded_base[:100] + \"...\",\n",
        "        \"FT_Response_Snippet\": decoded_ft[:100] + \"...\",\n",
        "        \"Base_Score\": score_base,\n",
        "        \"FT_Score\": score_ft,\n",
        "        \"Improvement\": score_ft - score_base\n",
        "    })\n",
        "\n",
        "\n",
        "print(\"\\n \\nüöÄ Finished Evaluation Loop...\\n\")\n"
      ],
      "metadata": {
        "id": "MH_VDlaeF9GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# PART 6: RESULTS & ANALYSIS\n",
        "# ==========================================\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\\n=========================================\")\n",
        "print(f\"üìä EVALUATION REPORT FOR: {MODEL_NAME}\")\n",
        "print(\"=========================================\")\n",
        "print(f\"Average Base Model Score:       {df['Base_Score'].mean():.2f}\")\n",
        "print(f\"Average Fine-Tuned Model Score: {df['FT_Score'].mean():.2f}\")\n",
        "print(\"=========================================\\n\")\n",
        "\n",
        "# Display the detailed dataframe\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "display(df[[\"Scenario\", \"Base_Score\", \"FT_Score\", \"Improvement\"]])\n",
        "\n",
        "print(\"\\n\\nüîç DETAILED COMPARISON (First Example):\")\n",
        "print(f\"Q: {df.iloc[0]['Scenario']}\")\n",
        "print(f\"\\n--- BASE MODEL ({df.iloc[0]['Base_Score']}) ---\")\n",
        "print(results[0]['Base_Response_Snippet'])\n",
        "print(f\"\\n--- STOIC MODEL ({df.iloc[0]['FT_Score']}) ---\")\n",
        "print(results[0]['FT_Response_Snippet'])"
      ],
      "metadata": {
        "id": "_NSeG7khF-UG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
